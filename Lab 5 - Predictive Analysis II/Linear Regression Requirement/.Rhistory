summary(lm_model)
#(Q15) What is the R-squared value here ? What does R-squared indicate?
print("R-squared value:0.8532")
print("It indicates that 85.42% of the variance in the dependent variable LungCap is
explained by the independent variables(Used by the model). In other words.
If we draw the linear Model plane in the space and the training points we will see the points are highly around the plane")
#(Q16) Show the coefficients of the linear model. Do they make sense?
#If not, which variables don't make sense? What should you do?
print(lm_model$coefficients)
print("Yes the Make Sense More Details are in the Report :D")
#(Q17) Redraw a scatter plot between Age and LungCap. Display/Overlay the linear model (a line) over it.
#Hint: Use the function abline(model, col="red").
#Note (1) : A warning will be displayed that this function will display only the first two
#           coefficients in the model. It's OK.
#Note (2) : If you are working correctly, the line will not be displayed on the plot. Why?
# Redraw scatter plot between Age and LungCap
plot(LungCap ~ Age, data = df, xlab = "Age", ylab = "LungCap", main = "Scatter Plot of Age vs. LungCap")
# Overlay linear model as a line
abline(lm_model, col = "red")
# Line Wasn't draw why?
#(Intercept)          Age
#-11.3224856    0.1605296
# Line to Draw LungCap=0.1605296*Age -11.3224856 [Refer to the Line Draw in The report]
print("The line of separator   is below the graph values we have its y-intercept -11.32 and x-intercept=70 so we cannot see it here :D")
#(Q18)Repeat Q13 but with these variables Age, Smoke and Cesarean as the only independent variables.
lm_model_2 <- lm(LungCap ~ Age + Smoke + Caesarean,data = df)
#(Q19)Repeat Q16, Q17 for the new model. What happened?
print(lm_model_2$coefficients)
print("Yes the Make Sense More Details are in the Report :D")
plot(LungCap ~ Age, data = df, xlab = "Age", ylab = "LungCap", main = "Scatter Plot of Age vs. LungCap")
# Overlay linear model as a line
abline(lm_model_2, col = "red")
#(Q20)Predict results for this regression line on the training data.
predictions <- predict(lm_model_2, newdata = df)
#(Q21)Calculate the mean squared error (MSE)of the training data.
# Calculate Mean Squared Error (MSE)
mse <- mean((df$LungCap - predictions)^2)
print(paste("Mean Squared Error (MSE) of the training data:", mse))
# [Extra] For Model 1
# Predict results for this regression line on the training data.
predictions_1 <- predict(lm_model, newdata = df)
# Calculate the mean squared error (MSE)of the training data.
# Calculate Mean Squared Error (MSE)
mse <- mean((df$LungCap - predictions_1)^2)
print(paste("Mean Squared Error (MSE) of the training data [Model 1]:", mse))
#setwd("~/LAB")
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#=============================Part(1)=====================================
x <- runif(100, 0, 10)     # 100 draws between 0 & 10
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
y_50 <- 5 + 6*x + rnorm(100, sd = 50)
y_0 <- 5 + 6*x + rnorm(100, sd = 0)
y_100 <- 5 + 6*x + rnorm(100, sd = 100)
#Plot it
plot (x,y,main = "y=5+6*x + rnorm(100, sd = 2)")
plot (x,y_50,main = "y=5+6*x + rnorm(100, sd = 50)")
plot (x,y_0,main = "y=5+6*x + rnorm(100, sd = 0)")
plot (x,y_100,main = "y=5+6*x + rnorm(100, sd = 100)")
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
model1_50 <- lm(y_50 ~ x)
model1_0 <- lm(y_0 ~ x)
model1_100 <- lm(y_100 ~ x)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data Learn about predict by saying ?predict.lm
ypred_50 <- predict(model1_50)
ypred_0 <- predict(model1_0)
ypred_100 <- predict(model1_100)
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
plot(y,y, type="l",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred)
plot(y,y, type="l",color="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y") # plotting the ideal line
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
plot(y,y, type="l",color="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y") # plotting the ideal line
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred)
par(mfrow=c(1,1))
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred_50)
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:0", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred_0)
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:100", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred_100)
par(mfrow=c(1,1))
points(y, ypred_100)
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:100", xlab="true y", ylab="predicted y") # plotting the ideal line
par(mfrow=c(1,1))
points(y, ypred_100)
par(mfrow=c(1,1))
points(y, ypred_100)
points(y, ypred_100)
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:100", xlab="true y", ylab="predicted y") # plotting the ideal line
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:100", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred_100)
# Detailed model results
d1 <- summary(model1)
print(model1)
# Detailed model results
d1_50 <- summary(model1_50)
print(model1_50)
# Detailed model results
d1_0 <- summary(model1_0)
print(model1_0)
# Detailed model results
d1_100 <- summary(model1_100)
print(model1_100)
# Calculate the range of both y and ypred
y_range <- range(c(y, ypred))
# Calculate the range of both y and ypred
y_range <- range(c(y, ypred))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y",xlim = y_range, ylim = y_range) # plotting the ideal line
points(y, ypred)
par(mfrow=c(1,1))
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
y_range <- range(c(y, ypred)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y",xlim = y_range, ylim = y_range) # plotting the ideal line
points(y, ypred)
par(mfrow=c(1,1))
y_range <- range(c(y, ypred)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y",xlim = y_range, ylim = y_range) # plotting the ideal line
points(y, ypred_50)
par(mfrow=c(1,1))
y_range <- range(c(y, ypred)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:0", xlab="true y", ylab="predicted y",xlim = y_range, ylim = y_range) # plotting the ideal line
points(y, ypred_0)
#setwd("~/LAB")
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#=============================Part(1)=====================================
x <- runif(100, 0, 10)     # 100 draws between 0 & 10
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
y_50 <- 5 + 6*x + rnorm(100, sd = 50)
y_0 <- 5 + 6*x + rnorm(100, sd = 0)
y_100 <- 5 + 6*x + rnorm(100, sd = 100)
#Plot it
plot (x,y,main = "y=5+6*x + rnorm(100, sd = 2)")
plot (x,y_50,main = "y=5+6*x + rnorm(100, sd = 50)")
plot (x,y_0,main = "y=5+6*x + rnorm(100, sd = 0)")
plot (x,y_100,main = "y=5+6*x + rnorm(100, sd = 100)")
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
model1_50 <- lm(y_50 ~ x)
model1_0 <- lm(y_0 ~ x)
model1_100 <- lm(y_100 ~ x)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data Learn about predict by saying ?predict.lm
ypred_50 <- predict(model1_50)
ypred_0 <- predict(model1_0)
ypred_100 <- predict(model1_100)
# Detailed model results
d1 <- summary(model1)
print(model1)
# Detailed model results
d1_50 <- summary(model1_50)
print(model1_50)
# Detailed model results
d1_0 <- summary(model1_0)
print(model1_0)
# Detailed model results
d1_100 <- summary(model1_100)
print(model1_100)
# Detailed model results
d1_100 <- summary(model1_100)
print(model1_100)
plot (x,y_100,main = "y=5+6*x + rnorm(100, sd = 100)")
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
model1_50 <- lm(y_50 ~ x)
model1_0 <- lm(y_0 ~ x)
model1_100 <- lm(y_100 ~ x)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data Learn about predict by saying ?predict.lm
ypred_50 <- predict(model1_50)
ypred_0 <- predict(model1_0)
ypred_100 <- predict(model1_100)
# Detailed model results
d1 <- summary(model1)
print(model1)
# Detailed model results
d1_50 <- summary(model1_50)
print(model1_50)
# Detailed model results
d1_0 <- summary(model1_0)
print(model1_0)
# Detailed model results
d1_100 <- summary(model1_100)
print(model1_100)
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
y_range <- range(c(y, ypred)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y",xlim = y_range, ylim = y_range) # plotting the ideal line
points(y, ypred)
par(mfrow=c(1,1))
y_range_50 <- range(c(y, ypred_50)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y",xlim = y_range_50, ylim = y_range_50) # plotting the ideal line
points(y, ypred_50)
par(mfrow=c(1,1))
y_range_0 <- range(c(y, ypred_0)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:0", xlab="true y", ylab="predicted y",xlim = y_range_0, ylim = y_range_0) # plotting the ideal line
points(y, ypred_0)
par(mfrow=c(1,1))
y_range_100 <- range(c(y, ypred_100)) # Calculate the range of both y and ypred
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:100", xlab="true y", ylab="predicted y",xlim = y_range_100, ylim = y_range_100) # plotting the ideal line
points(y, ypred_100)
#setwd("~/LAB")
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#=============================Part(1)=====================================
x <- runif(100, 0, 10)     # 100 draws between 0 & 10
y <- 5 + 6*x + rnorm(100, sd = 2)  # default values for rnorm (mean = 0 and sigma = 1)
y_50 <- 5 + 6*x + rnorm(100, sd = 50)
y_0 <- 5 + 6*x + rnorm(100, sd = 0)
y_100 <- 5 + 6*x + rnorm(100, sd = 100)
#Plot it
plot (x,y,main = "y=5+6*x + rnorm(100, sd = 2)")
plot (x,y_50,main = "y=5+6*x + rnorm(100, sd = 50)")
plot (x,y_0,main = "y=5+6*x + rnorm(100, sd = 0)")
plot (x,y_100,main = "y=5+6*x + rnorm(100, sd = 100)")
# OLS model
# OLS : Ordinary Least Squares
model1 <- lm(y ~ x)
model1_50 <- lm(y_50 ~ x)
model1_0 <- lm(y_0 ~ x)
model1_100 <- lm(y_100 ~ x)
# Regression diagnostics --
ypred <- predict(model1) # use the trained model to predict the same training data Learn about predict by saying ?predict.lm
ypred_50 <- predict(model1_50)
ypred_0 <- predict(model1_0)
ypred_100 <- predict(model1_100)
# Detailed model results
d1 <- summary(model1)
print(model1)
# Detailed model results
d1_50 <- summary(model1_50)
print(model1_50)
# Detailed model results
d1_0 <- summary(model1_0)
print(model1_0)
# Detailed model results
d1_100 <- summary(model1_100)
print(model1_100)
# plotting the predicted points the nearer to the ideal line the better
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:2", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred)
par(mfrow=c(1,1))
plot(y,y, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y, ypred_50)
plot(y_50,y_50, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y_50, ypred_50)
par(mfrow=c(1,1))
plot(y_50,y_50, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y") # plotting the ideal line
plot(y_50,y_50, type="l",col="red",main = "Predictions Model(1) std:50", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y_50, ypred_50)
par(mfrow=c(1,1))
plot(y_0,y_0, type="l",col="red",main = "Predictions Model(1) std:0", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y_0, ypred_0)
par(mfrow=c(1,1))
plot(y_100,y_100, type="l",col="red",main = "Predictions Model(1) std:100", xlab="true y", ylab="predicted y") # plotting the ideal line
points(y_100, ypred_100)
cat("OLS gave slope of ", d1$coefficients[2,1],
"and an R-sqr of ", d1$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1, 1,main = "Residual Plot For Model(1) std:2") # plot one diagnostic graphs
#(Q3) How is the value of R-squared affected by changing the value
#of standard deviation in Q1?
# Learn about this object by saying ?summary.lm and by saying str(d)
cat("OLS gave slope of ", d1$coefficients[2,1],
"and an R-sqr of ", d1$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1, 1,main = "Residual Plot For Model(1) std:2") # plot one diagnostic graphs
cat("OLS gave slope of ", d1_50$coefficients[2,1],
"and an R-sqr of ", d1_50$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1_50, 1,main = "Residual Plot For Model(1) std:50") # plot one diagnostic graphs
cat("OLS gave slope of ", d1_0$coefficients[2,1],
"and an R-sqr of ", d1_0$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1_0, 1,main = "Residual Plot For Model(1) std:0") # plot one diagnostic graphs
cat("OLS gave slope of ", d1_100$coefficients[2,1],
"and an R-sqr of ", d1_100$r.squared, "\n")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1_100, 1,main = "Residual Plot For Model(1) std:100") # plot one diagnostic graphs
# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red")
#Graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model1, 1,main = "Residual Plot For Model(1) std:2") # plot one diagnostic graphs
#(Q4)What do you conclude about the residual plot? Is it a good residual plot?
print("Check Report")
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 100 + 6*x1 + 0.1*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
plot(x1,y1)
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
#y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",name="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
#plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
#plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
x1 <- runif(100)
x1 <- runif(100)
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
#plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
points(ytrue, ypred)
x1 <- runif(100)
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
#plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
#plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
points(ytrue, ypred)
x1 <- runif(100)
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
#plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#========================Part(2)=====================================================
rm(list=ls()) # clear the R environment by removing all objects from the workspace
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
#plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 20*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
#plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
points(ytrue, ypred)
# graphic dignostic (cont.)
par(mfrow=c(1,1)) # parameters for the next plot
plot(model, 1) # plot the diagnostic graphs
#Training a linear regression model
x1 <- runif(100)
# introduce a slight nonlinearity
#(A)
#y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)
#plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
y1 = 5 + 6*x1 + 1*x1*x1 + rnorm(100)
plot(x1,y1,main = "Training Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
model <- lm(y1 ~ x1)
summary(model)
x1 <- runif(100)
#(B)
#ytrue = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)  # same equation of y1 but on xtest to get true y for xtest
ytrue = 5 + 6*x1 + 1*x1*x1 + rnorm(100)
ypred <- predict(model, data.frame(x1))
par(mfrow=c(1,1))
#plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 0.1*x1*x1 + rnorm(100)")
plot(ytrue, ytrue, type="l", xlab="true y", ylab="predicted y",main="Testing Data Points y1 = 5 + 6*x1 + 20*x1*x1 + rnorm(100)")
points(ytrue, ypred)
